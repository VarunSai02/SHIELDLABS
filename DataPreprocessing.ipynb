{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02364e92-ecb2-4e22-b23f-b29649991b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load clinical and omics data\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "mrna_data = pd.read_csv('data_mrna_seq_v2_rsem.csv')\n",
    "methylation_data = pd.read_csv('data_methylation_hm27_hm450_merged.csv')\n",
    "cna_data = pd.read_csv('data_log2_cna.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b40425-afcc-409c-966a-c472dcd0dee9",
   "metadata": {},
   "source": [
    "Transpose files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a36005e-197d-4f9a-9830-97020a134f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of missing values before removing columns in the data_cna DataFrame: 0\n",
      "\n",
      "Total number of missing values after removing columns in the data_cna DataFrame: 0\n",
      "Transposed data (columns with no missing values) saved as 'data_log2_cna.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CNA data file\n",
    "cna_data = pd.read_csv('data_log2_cna.csv')\n",
    "\n",
    "# Separate the metadata columns from sample data\n",
    "metadata = cna_data[['Hugo_Symbol', 'Entrez_Gene_Id', 'Cytoband']]\n",
    "sample_data = cna_data.drop(columns=['Hugo_Symbol', 'Entrez_Gene_Id', 'Cytoband'])\n",
    "\n",
    "# Transpose the sample data\n",
    "sample_data_transposed = sample_data.T\n",
    "\n",
    "# Set sample IDs as a column (originally column headers), and reset the index\n",
    "sample_data_transposed.columns = metadata['Hugo_Symbol']  # Use Hugo_Symbol as column headers\n",
    "sample_data_transposed.index.name = 'Sample_ID'\n",
    "sample_data_transposed.reset_index(inplace=True)\n",
    "\n",
    "# Print the total number of missing values before removing columns\n",
    "total_missing_before = sample_data_transposed.isnull().sum().sum()\n",
    "print(f\"\\nTotal number of missing values before removing columns in the data_cna DataFrame: {total_missing_before}\")\n",
    "\n",
    "# Remove columns that have ANY missing data\n",
    "sample_data_transposed = sample_data_transposed.loc[:, sample_data_transposed.isnull().sum() == 0]\n",
    "\n",
    "# Print the total number of missing values after dropping columns\n",
    "total_missing_after = sample_data_transposed.isnull().sum().sum()\n",
    "print(f\"\\nTotal number of missing values after removing columns in the data_cna DataFrame: {total_missing_after}\")\n",
    "\n",
    "# Save the transposed CNA data if needed\n",
    "sample_data_transposed.to_csv('data_log2_cna.csv', index=False)\n",
    "print(\"Transposed data (columns with no missing values) saved as 'data_log2_cna.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ad17a5-b3da-4d6c-9608-fb3e0beab546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed data saved as 'data_mrna_seq_v2_rsem.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the mrna data file\n",
    "mrna_data = pd.read_csv('data_mrna_seq_v2_rsem.csv')\n",
    "\n",
    "# Separate the metadata columns from sample data\n",
    "metadata = mrna_data[['Hugo_Symbol', 'Entrez_Gene_Id']]\n",
    "sample_data = mrna_data.drop(columns=['Hugo_Symbol', 'Entrez_Gene_Id'])\n",
    "\n",
    "# Transpose the sample data\n",
    "sample_data_transposed = sample_data.T\n",
    "\n",
    "# Set sample IDs as a column (originally column headers), and reset the index\n",
    "sample_data_transposed.columns = metadata['Hugo_Symbol']  # Use Hugo_Symbol as column headers\n",
    "sample_data_transposed.index.name = 'Sample_ID'\n",
    "sample_data_transposed.reset_index(inplace=True)\n",
    "\n",
    "# Save the transposed mrna data if needed\n",
    "sample_data_transposed.to_csv('data_mrna_seq_v2_rsem.csv', index=False)\n",
    "print(\"Transposed data saved as 'data_mrna_seq_v2_rsem.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee7f5ab-df42-411d-b574-d27abf2c160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed and saved the methylation data file.\n"
     ]
    }
   ],
   "source": [
    "# Load the DNA methylation data\n",
    "methylation_data = pd.read_csv('data_methylation_hm27_hm450_merged.csv')\n",
    "\n",
    "# Drop metadata columns\n",
    "methylation_data = methylation_data.drop(columns=['ENTITY_STABLE_ID', 'DESCRIPTION', 'TRANSCRIPT_ID'])\n",
    "\n",
    "# Transpose the data so that samples become rows and CpG sites become columns\n",
    "# Set ENTITY_STABLE_ID as the index before transposing\n",
    "methylation_data = methylation_data.set_index('NAME').T\n",
    "\n",
    "# Rename the index to Sample_ID and reset the index to make Sample_ID a column\n",
    "methylation_data.index.name = 'Sample_ID'\n",
    "methylation_data.reset_index(inplace=True)\n",
    "\n",
    "# Save the transposed DataFrame back to the original file\n",
    "methylation_data.to_csv('data_methylation_hm27_hm450_merged.csv', index=False)\n",
    "print(\"Transposed and saved the methylation data file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5ef52e-a074-4de4-8d71-d1d02e98de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of missing values before dropping columns in the data_mrna_seq_v2_rsem.csv DataFrame: 1070496\n",
      "Total number of missing values after dropping columns in the data_mrna_seq_v2_rsem.csv DataFrame: 0\n",
      "Normalized and saved file: data_mrna_seq_v2_rsem.csv\n",
      "Total number of missing values after normalization in the data_mrna_seq_v2_rsem.csv DataFrame: 0\n",
      "\n",
      "Total number of missing values before dropping columns in the data_methylation_hm27_hm450_merged.csv DataFrame: 3247\n",
      "Total number of missing values after dropping columns in the data_methylation_hm27_hm450_merged.csv DataFrame: 0\n",
      "Normalized and saved file: data_methylation_hm27_hm450_merged.csv\n",
      "Total number of missing values after normalization in the data_methylation_hm27_hm450_merged.csv DataFrame: 0\n",
      "\n",
      "Total number of missing values before dropping columns in the data_log2_cna.csv DataFrame: 0\n",
      "Total number of missing values after dropping columns in the data_log2_cna.csv DataFrame: 0\n",
      "Normalized and saved file: data_log2_cna.csv\n",
      "Total number of missing values after normalization in the data_log2_cna.csv DataFrame: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# List of omics data files that have already been transposed\n",
    "omics_files = [\n",
    "    'data_mrna_seq_v2_rsem.csv', \n",
    "    'data_methylation_hm27_hm450_merged.csv',\n",
    "    'data_log2_cna.csv'\n",
    "]\n",
    "\n",
    "# Load, remove columns with missing values, normalize, and save each file\n",
    "for file in omics_files:\n",
    "    # Load the transposed data\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Print the total missing values before dropping columns\n",
    "    total_missing_before = df.isnull().sum().sum()\n",
    "    print(f\"\\nTotal number of missing values before dropping columns in the {file} DataFrame: {total_missing_before}\")\n",
    "    \n",
    "    # Remove columns that have ANY missing data\n",
    "    df = df.loc[:, df.isnull().sum() == 0]\n",
    "    \n",
    "    # Print the total missing values after dropping columns\n",
    "    total_missing_after_drop = df.isnull().sum().sum()\n",
    "    print(f\"Total number of missing values after dropping columns in the {file} DataFrame: {total_missing_after_drop}\")\n",
    "    \n",
    "    # Apply StandardScaler for normalization on all columns except 'Sample_ID'\n",
    "    scaler = StandardScaler()\n",
    "    df.iloc[:, 1:] = scaler.fit_transform(df.iloc[:, 1:])  # Skip the 'Sample_ID' column\n",
    "    \n",
    "    # Save the normalized DataFrame back to the original file\n",
    "    df.to_csv(file, index=False)\n",
    "    print(f\"Normalized and saved file: {file}\")\n",
    "    \n",
    "    # Print the total missing values after normalization\n",
    "    total_missing_after_norm = df.isnull().sum().sum()\n",
    "    print(f\"Total number of missing values after normalization in the {file} DataFrame: {total_missing_after_norm}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ae5ec5c-0270-4b49-ae43-2fe0f464a049",
   "metadata": {},
   "source": [
    "Preprocess clinical data\n",
    "\n",
    "delete 1st 4 rows as it is meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2fbe1f2-4c3e-415d-859a-ed10188ceac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to 'data_clinical_patient.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "patient_data = pd.read_csv('data_clinical_patient.csv')\n",
    "sample_data = pd.read_csv('data_clinical_sample.csv')\n",
    "\n",
    "# Merge data_clinical_patient with the cleaned clinical_sample_data based on PATIENT_ID\n",
    "merged_data = pd.merge(patient_data, sample_data, on='PATIENT_ID', how='inner')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_data.head()\n",
    "\n",
    "# Optionally, save the merged data to a CSV file\n",
    "merged_data.to_csv('data_clinical_patient.csv', index=False)\n",
    "print(\"Merged data saved to 'data_clinical_patient.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89429160-3760-4767-b421-9974a3ceb294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty columns dropped and data saved to 'data_clinical_patient.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Drop empty columns\n",
    "# Load the data_clinical_patient file\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "\n",
    "# Drop columns that are entirely NaN\n",
    "clinical_data = clinical_data.dropna(axis=1, how='all')\n",
    "\n",
    "# Save the cleaned DataFrame back to the same file\n",
    "clinical_data.to_csv('data_clinical_patient.csv', index=False)\n",
    "print(\"Empty columns dropped and data saved to 'data_clinical_patient.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6006a721-286f-4019-b4b3-1fecb3ca8edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\AppData\\Local\\Temp\\ipykernel_27616\\2183474512.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  clinical_data['Surgery_Type'].fillna('Unknown', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final clinical data with treatment flags saved to 'data_clinical_patient.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------ 1. Load Data Files ------------------ #\n",
    "# Load clinical data (contains at least Patient_ID and Sample_ID)\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "\n",
    "# Load sample acquisition data (has Sample_ID, METHOD_OF_SAMPLE_PROCUREMENT, and START_DATE as numeric surgery date)\n",
    "sample_acq = pd.read_csv('data_timeline_sample_acquisition.csv')\n",
    "\n",
    "# Load treatment timeline data (has Patient_ID, START_DATE as numeric treatment start date, TREATMENT_TYPE, and RADIATION_TYPE)\n",
    "treatment = pd.read_csv('data_timeline_treatment.csv')\n",
    "\n",
    "# ------------------ 2. Merge Surgery Information ------------------ #\n",
    "# Merge sample acquisition info (surgery type and numeric surgery date) into clinical_data using Sample_ID.\n",
    "clinical_data = clinical_data.merge(\n",
    "    sample_acq[['Sample_ID', 'METHOD_OF_SAMPLE_PROCUREMENT', 'START_DATE']],\n",
    "    on='Sample_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename columns for clarity: rename METHOD_OF_SAMPLE_PROCUREMENT to Surgery_Type and START_DATE to Surgery_Date.\n",
    "clinical_data.rename(columns={\n",
    "    'METHOD_OF_SAMPLE_PROCUREMENT': 'Surgery_Type',\n",
    "    'START_DATE': 'Surgery_Date'\n",
    "}, inplace=True)\n",
    "\n",
    "# If Surgery_Type is missing, fill it with \"Unknown\"\n",
    "clinical_data['Surgery_Type'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# ------------------ 3. Define Function to Derive Treatment Flags ------------------ #\n",
    "def get_treatment_flags(row, treatment_df):\n",
    "    \"\"\"\n",
    "    For a given clinical record (row), compute six flags based on treatment data:\n",
    "      - Chemo_Given: 1 if any chemotherapy event is recorded.\n",
    "      - Chemo_within_6wks: 1 if any chemotherapy start date is within 42 days of surgery.\n",
    "      - Radiation_Internal_Given: 1 if any internal radiation event is recorded.\n",
    "      - Radiation_Internal_within_6wks: 1 if internal radiation started within 42 days of surgery.\n",
    "      - Radiation_External_Given: 1 if any external radiation event is recorded.\n",
    "      - Radiation_External_within_6wks: 1 if external radiation started within 42 days of surgery.\n",
    "    \n",
    "    Assumes both Surgery_Date and treatment START_DATE are numeric values (e.g., days from a reference).\n",
    "    \"\"\"\n",
    "    patient_id = row['PATIENT_ID']\n",
    "    surgery_date = row['Surgery_Date']  # Numeric surgery date\n",
    "    \n",
    "    # If surgery date is missing, return zeros.\n",
    "    if pd.isna(surgery_date):\n",
    "        return pd.Series({\n",
    "            'Chemo_Given': 0,\n",
    "            'Chemo_within_6wks': 0,\n",
    "            'Radiation_Internal_Given': 0,\n",
    "            'Radiation_Internal_within_6wks': 0,\n",
    "            'Radiation_External_Given': 0,\n",
    "            'Radiation_External_within_6wks': 0\n",
    "        })\n",
    "    \n",
    "    # Filter treatment events for this patient.\n",
    "    patient_treatments = treatment_df[treatment_df['PATIENT_ID'] == patient_id]\n",
    "    \n",
    "    # Initialize flags.\n",
    "    chemo_given = 0\n",
    "    chemo_within = 0\n",
    "    rad_internal_given = 0\n",
    "    rad_internal_within = 0\n",
    "    rad_external_given = 0\n",
    "    rad_external_within = 0\n",
    "    \n",
    "    # Define the threshold for \"within 6 weeks\" as 42 days.\n",
    "    threshold = 42\n",
    "    \n",
    "    for _, tr in patient_treatments.iterrows():\n",
    "        treat_type = str(tr['TREATMENT_TYPE']).lower()  # Normalize treatment type text.\n",
    "        treat_date = tr['START_DATE']  # Numeric treatment start date.\n",
    "        \n",
    "        # Calculate the time difference.\n",
    "        if pd.notna(treat_date) and pd.notna(surgery_date):\n",
    "            diff = treat_date - surgery_date\n",
    "        else:\n",
    "            diff = None\n",
    "        \n",
    "        # Check for chemotherapy.\n",
    "        if 'chemo' in treat_type:\n",
    "            chemo_given = 1\n",
    "            if diff is not None and diff >= 0 and diff <= threshold:\n",
    "                chemo_within = 1\n",
    "        \n",
    "        # Check for radiation treatment.\n",
    "        elif 'radiation' in treat_type:\n",
    "            rad_type = str(tr['RADIATION_TYPE']).lower() if pd.notna(tr['RADIATION_TYPE']) else \"\"\n",
    "            if 'internal' in rad_type:\n",
    "                rad_internal_given = 1\n",
    "                if diff is not None and diff >= 0 and diff <= threshold:\n",
    "                    rad_internal_within = 1\n",
    "            elif 'external' in rad_type:\n",
    "                rad_external_given = 1\n",
    "                if diff is not None and diff >= 0 and diff <= threshold:\n",
    "                    rad_external_within = 1\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Chemo_Given': chemo_given,\n",
    "        'Chemo_within_6wks': chemo_within,\n",
    "        'Radiation_Internal_Given': rad_internal_given,\n",
    "        'Radiation_Internal_within_6wks': rad_internal_within,\n",
    "        'Radiation_External_Given': rad_external_given,\n",
    "        'Radiation_External_within_6wks': rad_external_within\n",
    "    })\n",
    "\n",
    "# ------------------ 4. Apply the Treatment Flag Function ------------------ #\n",
    "# Apply the function row by row to compute the treatment flags for each clinical record.\n",
    "treatment_flags = clinical_data.apply(lambda row: get_treatment_flags(row, treatment), axis=1)\n",
    "clinical_data = pd.concat([clinical_data, treatment_flags], axis=1)\n",
    "\n",
    "# ------------------ 5. Remove the Surgery_Date Column ------------------ #\n",
    "# Drop the Surgery_Date column since you don't want it in the final clinical data.\n",
    "clinical_data.drop(columns=['Surgery_Date'], inplace=True)\n",
    "\n",
    "# ------------------ 6. Save the Final Clinical Data ------------------ #\n",
    "# Save the final clinical dataset that now includes:\n",
    "# - Surgery_Type (from sample acquisition, with missing filled as \"Unknown\")\n",
    "# - Treatment flags (Chemo_Given, Chemo_within_6wks, Radiation_Internal_Given, Radiation_Internal_within_6wks, \n",
    "#   Radiation_External_Given, Radiation_External_within_6wks)\n",
    "clinical_data.to_csv('data_clinical_patient.csv', index=False)\n",
    "print(\"Final clinical data with treatment flags saved to 'data_clinical_patient.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed67334-753e-4b65-8280-93e9811a7d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cfeb8fae-e583-42c7-bf30-2e8c118df20a",
   "metadata": {},
   "source": [
    "Split Descriptive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d1b22ff-c551-4d3f-99bb-268f4d05c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values with descriptions have been split, and data saved to 'data_clinical_patient.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the clinical data\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "\n",
    "# Identify columns that need to be split (columns with values like '0:DiseaseFree')\n",
    "columns_to_split = ['PFS_STATUS']  # Add other relevant columns\n",
    "\n",
    "# Split the values in the specified columns and keep only the numeric/categorical part\n",
    "for col in columns_to_split:\n",
    "    clinical_data[col] = clinical_data[col].astype(str).str.split(':').str[0]\n",
    "\n",
    "# Convert columns to appropriate types (e.g., integer if applicable)\n",
    "for col in columns_to_split:\n",
    "    clinical_data[col] = pd.to_numeric(clinical_data[col], errors='coerce')\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "clinical_data.to_csv('data_clinical_patient.csv', index=False)\n",
    "print(\"Values with descriptions have been split, and data saved to 'data_clinical_patient.csv'.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41a1357a-6327-4467-99be-b7e5d6c49e70",
   "metadata": {},
   "source": [
    "Fill DFS_STATUS based on other columns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c6dfb81-cea6-4c45-8a5c-af10969e54ab",
   "metadata": {},
   "source": [
    "PERSON_NEOPLASM_CANCER_STATUS:\n",
    "\n",
    "Based on: OS_MONTHS and DSS_STATUS.\n",
    "Logic: If OS_MONTHS is 0 and DSS_STATUS indicates DECEASED, you can fill OS_STATUS with DECEASED.\n",
    "DAYS_TO_BIRTH:\n",
    "\n",
    "Based on: AGE.\n",
    "Logic: If AGE is provided, you could estimate DAYS_TO_BIRTH by multiplying AGE by 365.25 (approximate days in a year).\n",
    "NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT:\n",
    "\n",
    "Based on: DFS_STATUS, PFS_STATUS.\n",
    "Logic: If DFS_STATUS or PFS_STATUS indicates recurrence or progression, you could infer NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT as YES.\n",
    "PFS_STATUS:\n",
    "\n",
    "Based on: DFS_STATUS and NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT.\n",
    "Logic: If DFS_STATUS indicates progression or NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT is YES, you can fill PFS_STATUS as 1 (progressed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736a3d6-ce00-4baa-bb5a-78a970506cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "004584ba-e47c-4e7b-b5df-5d1a4c4bcd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed missing values and saved to 'data_clinical_patient.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the clinical data\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "\n",
    "# Fill PERSON_NEOPLASM_CANCER_STATUS based on DFS_STATUS and PFS_STATUS\n",
    "clinical_data['PERSON_NEOPLASM_CANCER_STATUS'] = clinical_data['PERSON_NEOPLASM_CANCER_STATUS'].fillna(\n",
    "    clinical_data.apply(\n",
    "        lambda row: 'With Tumor' if row['DFS_STATUS'] == 1 or row['PFS_STATUS'] == 1 else row['PERSON_NEOPLASM_CANCER_STATUS'],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fill DAYS_TO_BIRTH based on AGE\n",
    "clinical_data['DAYS_TO_BIRTH'] = clinical_data['DAYS_TO_BIRTH'].fillna(\n",
    "    clinical_data['AGE'].apply(lambda x: -x * 365.25 if pd.notna(x) else pd.NA)\n",
    ")\n",
    "\n",
    "# Fill NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT based on DFS_STATUS and PFS_STATUS\n",
    "clinical_data['NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT'] = clinical_data['NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT'].fillna(\n",
    "    clinical_data.apply(\n",
    "        lambda row: 'YES' if row['DFS_STATUS'] == 1 or row['PFS_STATUS'] == 1 else row['NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT'],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame\n",
    "clinical_data.to_csv('data_clinical_patient.csv', index=False)\n",
    "print(\"Imputed missing values and saved to 'data_clinical_patient.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b34efea-9dfb-4063-8c9c-81d9af9129f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned clinical data saved, with irrelevant columns removed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "\n",
    "# List of metadata or irrelevant columns\n",
    "columns_to_drop = [\n",
    "    'OTHER_PATIENT_ID', 'FORM_COMPLETION_DATE', 'ICD_10', \n",
    "    'ICD_O_3_HISTOLOGY', 'ICD_O_3_SITE', 'INFORMED_CONSENT_VERIFIED', 'IN_PANCANPATHWAYS_FREEZE',\n",
    "    'TISSUE_PROSPECTIVE_COLLECTION_INDICATOR',\n",
    "    'AJCC_STAGING_EDITION',\n",
    "    'TISSUE_RETROSPECTIVE_COLLECTION_INDICATOR',\n",
    "    'TISSUE_SOURCE_SITE_CODE',\n",
    "    'TISSUE_SOURCE_SITE',\n",
    "    'SAMPLE_TYPE',\n",
    "    'CANCER_TYPE', 'CANCER_TYPE_ACRONYM', # We have cancer type detailed\n",
    "    'SOMATIC_STATUS',\n",
    "    'PATIENT_ID',\n",
    "    'DAYS_LAST_FOLLOWUP',\n",
    "    'DAYS_TO_INITIAL_PATHOLOGIC_DIAGNOSIS',\n",
    "    'OS_STATUS',\n",
    "    'OS_MONTHS',\n",
    "    'DSS_STATUS',\n",
    "    'DSS_MONTHS',\n",
    "    'DFS_MONTHS',\n",
    "    'DFS_STATUS',\n",
    "    'SEX',\n",
    "    'NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT'\n",
    "]\n",
    "\n",
    "# Drop the irrelevant columns\n",
    "clinical_data_cleaned = clinical_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save the cleaned DataFrame back to the same file\n",
    "clinical_data_cleaned.to_csv('data_clinical_patient.csv', index=False)\n",
    "print(\"Cleaned clinical data saved, with irrelevant columns removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec8858e8-4787-4593-9200-d1ea2e94e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Categories:\n",
      "Recurrence_Category\n",
      "No recurrence yet, but follow-up ≤ 60 months    320\n",
      "Recurred/Progressed within 60 months            114\n",
      "No recurrence with > 60 months follow-up         89\n",
      "Recurred/Progressed after 60 months               5\n",
      "Unknown/Missing                                   1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binary Classification:\n",
      "Recurrence_Binary\n",
      "0    415\n",
      "1    114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Load and preprocess the data\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "\n",
    "# Convert Disease Free Months to numeric\n",
    "clinical_data['PFS_MONTHS'] = pd.to_numeric(clinical_data['PFS_MONTHS'], errors='coerce')\n",
    "\n",
    "# Create a new column for clear categorization\n",
    "def categorize_recurrence(row):\n",
    "    if pd.isna(row['PFS_STATUS']) or pd.isna(row['PFS_MONTHS']):\n",
    "        return 'Unknown/Missing'\n",
    "    elif row['PFS_STATUS'] == 1 and row['PFS_MONTHS'] <= 60:\n",
    "        return 'Recurred/Progressed within 60 months'\n",
    "    elif row['PFS_STATUS'] == 1 and row['PFS_MONTHS'] > 60:\n",
    "        return 'Recurred/Progressed after 60 months'\n",
    "    elif row['PFS_STATUS'] == 0 and row['PFS_MONTHS'] <= 60:\n",
    "        return 'No recurrence yet, but follow-up ≤ 60 months'\n",
    "    elif row['PFS_STATUS'] == 0 and row['PFS_MONTHS'] > 60:\n",
    "        return 'No recurrence with > 60 months follow-up'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "clinical_data['Recurrence_Category'] = clinical_data.apply(categorize_recurrence, axis=1)\n",
    "\n",
    "# For binary classification (what you'll likely want for modeling)\n",
    "clinical_data['Recurrence_Binary'] = clinical_data.apply(\n",
    "    lambda row: 1 if (row['PFS_STATUS'] == 1 and row['PFS_MONTHS'] <= 60) else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display the distributions\n",
    "print(\"Detailed Categories:\")\n",
    "print(clinical_data['Recurrence_Category'].value_counts())\n",
    "print(\"\\nBinary Classification:\")\n",
    "print(clinical_data['Recurrence_Binary'].value_counts())\n",
    "\n",
    "# Save categorized data if needed\n",
    "clinical_data.to_csv('categorized_clinical_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "113244bc-fa8c-4d8c-b468-45d964fc3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 203 rows, removed 326 rows (categories: ['No recurrence yet, but follow-up ≤ 60 months', 'Recurred/Progressed after 60 months', 'Unknown/Missing']).\n",
      "Number of valid sample IDs: 203\n",
      "Filtered clinical data saved to 'data_clinical_patient.csv'\n",
      "data_mrna_seq_v2_rsem.csv: Removed 325 rows; kept 202 rows.\n",
      "Filtered omics data saved to 'data_mrna_seq_v2_rsem.csv'\n",
      "data_methylation_hm27_hm450_merged.csv: Removed 326 rows; kept 203 rows.\n",
      "Filtered omics data saved to 'data_methylation_hm27_hm450_merged.csv'\n",
      "data_log2_cna.csv: Removed 323 rows; kept 200 rows.\n",
      "Filtered omics data saved to 'data_log2_cna.csv'\n",
      "All datasets have been filtered to remove 'No recurrence yet, but follow-up ≤ 60 months' and 'Unknown/Missing'.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the clinical data\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "\n",
    "# Make sure PFS_MONTHS is numeric if needed\n",
    "clinical_data['PFS_MONTHS'] = pd.to_numeric(clinical_data['PFS_MONTHS'], errors='coerce')\n",
    "\n",
    "# 2. Apply categorization function\n",
    "clinical_data['Recurrence_Category'] = clinical_data.apply(categorize_recurrence, axis=1)\n",
    "\n",
    "# 3. Define the categories you want to remove\n",
    "categories_to_remove = [\n",
    "    \"No recurrence yet, but follow-up ≤ 60 months\",\n",
    "    \"Recurred/Progressed after 60 months\",\n",
    "    \"Unknown/Missing\"\n",
    "]\n",
    "\n",
    "# 4. Build a condition to KEEP rows NOT in those categories\n",
    "keep_condition = ~clinical_data['Recurrence_Category'].isin(categories_to_remove)\n",
    "\n",
    "clinical_data_filtered = clinical_data[keep_condition].copy()\n",
    "\n",
    "num_kept = len(clinical_data_filtered)\n",
    "num_removed = len(clinical_data) - num_kept\n",
    "print(f\"Kept {num_kept} rows, removed {num_removed} rows (categories: {categories_to_remove}).\")\n",
    "\n",
    "# 5. Extract valid Sample IDs\n",
    "valid_sample_ids = clinical_data_filtered['Sample_ID'].unique()\n",
    "print(f\"Number of valid sample IDs: {len(valid_sample_ids)}\")\n",
    "\n",
    "# Remove the Recurrence_Category column after filtering\n",
    "clinical_data_filtered.drop(columns=['Recurrence_Category'], inplace=True)\n",
    "\n",
    "# 6. Save the filtered clinical data\n",
    "clinical_data_filtered.to_csv('data_clinical_patient.csv', index=False)\n",
    "print(\"Filtered clinical data saved to 'data_clinical_patient.csv'\")\n",
    "\n",
    "# 7. Filter each omics dataset\n",
    "omics_files = [\n",
    "    'data_mrna_seq_v2_rsem.csv', \n",
    "    'data_methylation_hm27_hm450_merged.csv',\n",
    "    'data_log2_cna.csv'\n",
    "]\n",
    "\n",
    "for file in omics_files:\n",
    "    omics_df = pd.read_csv(file)\n",
    "    before_count = len(omics_df)\n",
    "    \n",
    "    # Keep only rows whose Sample_ID is in the filtered clinical data\n",
    "    omics_df = omics_df[omics_df['Sample_ID'].isin(valid_sample_ids)].copy()\n",
    "    after_count = len(omics_df)\n",
    "    \n",
    "    print(f\"{file}: Removed {before_count - after_count} rows; kept {after_count} rows.\")\n",
    "    \n",
    "    # Save the filtered omics data\n",
    "    omics_df.to_csv(file, index=False)\n",
    "    print(f\"Filtered omics data saved to '{file}'\")\n",
    "\n",
    "print(\"All datasets have been filtered to remove 'No recurrence yet, but follow-up ≤ 60 months' and 'Unknown/Missing'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "294e1b4d-7824-4cbd-9bd5-10e85147118f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to 'data_clinical_patient_preprocessed.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------- 1. Load & Filter Clinical Data -------------------- #\n",
    "clinical_data = pd.read_csv('data_clinical_patient.csv')\n",
    "\n",
    "# If PFS_MONTHS is critical, remove rows missing it\n",
    "if clinical_data['PFS_MONTHS'].isnull().any():\n",
    "    print(\"Warning: 'PFS_MONTHS' contains missing values. These rows will be excluded.\")\n",
    "    clinical_data = clinical_data.dropna(subset=['PFS_MONTHS'])\n",
    "\n",
    "# Ensure PFS_MONTHS is numeric\n",
    "clinical_data['PFS_MONTHS'] = pd.to_numeric(clinical_data['PFS_MONTHS'], errors='coerce')\n",
    "\n",
    "# -------------------- 2. Create Survival_Label from PFS -------------------- #\n",
    "# If patient progresses by 60 months => label = 1, otherwise 0\n",
    "clinical_data['Survival_Label'] = clinical_data['PFS_MONTHS'].apply(\n",
    "    lambda x: 1 if x <= 60 else 0\n",
    ")\n",
    "\n",
    "# -------------------- 3. (Optional) Drop Unused Columns -------------------- #\n",
    "# If you do not want to keep DFS_MONTHS, OS_MONTHS, or OS_STATUS as features:\n",
    "# clinical_data.drop(columns=['DFS_MONTHS', 'OS_MONTHS', 'OS_STATUS'], inplace=True)\n",
    "\n",
    "# -------------------- 4. Identify Categorical vs Numerical -------------------- #\n",
    "categorical_columns_numeric = ['Chemo_Given', \n",
    "                               'Chemo_within_6wks', 'Radiation_Internal_Given',\n",
    "                               'Radiation_Internal_within_6wks', 'Radiation_External_Given',\n",
    "                               'Radiation_External_within_6wks']\n",
    "categorical_columns = [\n",
    "    'SUBTYPE', 'ETHNICITY', 'HISTORY_NEOADJUVANT_TRTYN', 'PERSON_NEOPLASM_CANCER_STATUS',\n",
    "    'PRIOR_DX', 'RACE', 'RADIATION_THERAPY', 'GENETIC_ANCESTRY_LABEL', 'ONCOTREE_CODE',\n",
    "    'CANCER_TYPE_DETAILED', 'TUMOR_TYPE', 'GRADE', 'TUMOR_TISSUE_SITE', 'Surgery_Type'\n",
    "] + categorical_columns_numeric\n",
    "\n",
    "# Exclude the Survival_Label from one-hot encoding\n",
    "categorical_columns = [col for col in categorical_columns if col != 'Survival_Label']\n",
    "\n",
    "# -------------------- 5. Exclude Purely Binary Columns from One-Hot Encoding -------------------- #\n",
    "binary_columns = []\n",
    "for col in categorical_columns:\n",
    "    unique_vals = clinical_data[col].dropna().unique()\n",
    "    if len(unique_vals) == 2 and set(unique_vals).issubset({0,1}):\n",
    "        binary_columns.append(col)\n",
    "\n",
    "multi_category_columns = list(set(categorical_columns) - set(binary_columns))\n",
    "\n",
    "# -------------------- 6. One-Hot Encode Only Multi-Category Columns -------------------- #\n",
    "clinical_data = pd.get_dummies(clinical_data, columns=multi_category_columns)\n",
    "\n",
    "# -------------------- 7. KNN Imputation for Numerical Columns -------------------- #\n",
    "# Identify numeric columns that might need KNN Imputation\n",
    "numerical_columns = clinical_data.select_dtypes(include=['number']).columns.difference(categorical_columns_numeric)\n",
    "\n",
    "# Ensure Survival_Label is NOT included in numerical columns for imputation or normalization\n",
    "numerical_columns = numerical_columns.difference(['Survival_Label'])\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "clinical_data[numerical_columns] = imputer.fit_transform(clinical_data[numerical_columns])\n",
    "\n",
    "# Ensure one-hot-encoded columns are integers (0 or 1)\n",
    "one_hot_encoded_columns = [\n",
    "    col for col in clinical_data.columns\n",
    "    if clinical_data[col].dropna().isin([0, 1]).all()\n",
    "]\n",
    "\n",
    "valid_binary_cols = []\n",
    "for col in one_hot_encoded_columns:\n",
    "    # Check if column has any NA/inf\n",
    "    if clinical_data[col].isna().any() or not clinical_data[col].isin([0, 1]).all():\n",
    "        print(f\"Dropping {col} because it has NA or non-0/1 values.\")\n",
    "    else:\n",
    "        valid_binary_cols.append(col)\n",
    "\n",
    "# Now cast only the valid binary columns\n",
    "clinical_data[valid_binary_cols] = clinical_data[valid_binary_cols].astype(int)\n",
    "\n",
    "# -------------------- 8. Normalize Numerical Data (EXCLUDING Survival_Label) -------------------- #\n",
    "scaler = StandardScaler()\n",
    "clinical_data[numerical_columns] = scaler.fit_transform(clinical_data[numerical_columns])\n",
    "\n",
    "# -------------------- 9. Save the Preprocessed Data -------------------- #\n",
    "clinical_data.to_csv('data_clinical_patient_preprocessed.csv', index=False)\n",
    "print(\"Preprocessed data saved to 'data_clinical_patient_preprocessed.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc6c0e6f-5c56-4002-81c4-21af012ae552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned clinical data saved, with irrelevant columns removed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "clinical_data = pd.read_csv('data_clinical_patient_preprocessed.csv')\n",
    "\n",
    "# List of metadata or irrelevant columns\n",
    "columns_to_drop = [\n",
    "    'PFS_MONTHS',\n",
    "    'PFS_STATUS'    \n",
    "]\n",
    "\n",
    "# Drop the irrelevant columns\n",
    "clinical_data_cleaned = clinical_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save the cleaned DataFrame back to the same file\n",
    "clinical_data_cleaned.to_csv('data_clinical_patient_preprocessed.csv', index=False)\n",
    "print(\"Cleaned clinical data saved, with irrelevant columns removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df5b5aed-7e27-489c-af52-4ec8d9eb6fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Data before cleaning: 0 missing values.\n",
      "Clinical Data after cleaning: 0 missing values.\n",
      "Clinical Data contains 203 unique Sample_IDs after cleaning.\n",
      "mRNA Data before cleaning: 0 missing values.\n",
      "mRNA Data after cleaning: 0 missing values.\n",
      "mRNA Data contains 202 unique Sample_IDs after cleaning.\n",
      "Methylation Data before cleaning: 0 missing values.\n",
      "Methylation Data after cleaning: 0 missing values.\n",
      "Methylation Data contains 203 unique Sample_IDs after cleaning.\n",
      "CNA Data before cleaning: 0 missing values.\n",
      "CNA Data after cleaning: 0 missing values.\n",
      "CNA Data contains 200 unique Sample_IDs after cleaning.\n",
      "All files have been cleaned to remove any missing values.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths for the filtered files (assuming these were saved previously)\n",
    "files = {\n",
    "    \"Clinical Data\": \"data_clinical_patient_preprocessed.csv\",\n",
    "    \"mRNA Data\": \"data_mrna_seq_v2_rsem.csv\",\n",
    "    \"Methylation Data\": \"data_methylation_hm27_hm450_merged.csv\",\n",
    "    \"CNA Data\": \"data_log2_cna.csv\"\n",
    "}\n",
    "\n",
    "for desc, file_path in files.items():\n",
    "    # Load the file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count total missing values in the file\n",
    "    total_missing_before = df.isnull().sum().sum()\n",
    "    print(f\"{desc} before cleaning: {total_missing_before} missing values.\")\n",
    "    \n",
    "    # Drop rows that contain any missing values\n",
    "    df_clean = df.dropna(how='any')\n",
    "    \n",
    "    # Count missing values after cleaning (should be zero)\n",
    "    total_missing_after = df_clean.isnull().sum().sum()\n",
    "    print(f\"{desc} after cleaning: {total_missing_after} missing values.\")\n",
    "    \n",
    "    # Optionally, print the number of unique Sample_IDs\n",
    "    if 'Sample_ID' in df_clean.columns:\n",
    "        unique_samples = df_clean['Sample_ID'].nunique()\n",
    "        print(f\"{desc} contains {unique_samples} unique Sample_IDs after cleaning.\")\n",
    "    \n",
    "    # Save the cleaned file (overwrite the original filtered file)\n",
    "    df_clean.to_csv(file_path, index=False)\n",
    "    \n",
    "print(\"All files have been cleaned to remove any missing values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c94fdd8a-10cb-4034-b7fa-0b81d6c84702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common Sample_IDs across all files: 199\n",
      "Clinical Data: Before filtering = 203 unique Sample_IDs, After filtering = 199 unique Sample_IDs.\n",
      "mRNA Data: Before filtering = 202 unique Sample_IDs, After filtering = 199 unique Sample_IDs.\n",
      "Methylation Data: Before filtering = 203 unique Sample_IDs, After filtering = 199 unique Sample_IDs.\n",
      "CNA Data: Before filtering = 200 unique Sample_IDs, After filtering = 199 unique Sample_IDs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths with descriptive names\n",
    "files = {\n",
    "    \"Clinical Data\": \"data_clinical_patient_preprocessed.csv\",\n",
    "    \"mRNA Data\": \"data_mrna_seq_v2_rsem.csv\",\n",
    "    \"Methylation Data\": \"data_methylation_hm27_hm450_merged.csv\",\n",
    "    \"CNA Data\": \"data_log2_cna.csv\"\n",
    "}\n",
    "\n",
    "# Dictionary to store unique Sample_IDs for each file\n",
    "sample_id_sets = {}\n",
    "\n",
    "# Load each file and extract unique Sample_IDs\n",
    "for name, file_path in files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'Sample_ID' in df.columns:\n",
    "        sample_id_sets[name] = set(df['Sample_ID'].unique())\n",
    "    else:\n",
    "        print(f\"{name} does not have a 'Sample_ID' column.\")\n",
    "\n",
    "# Compute the intersection of Sample_IDs across all files\n",
    "common_sample_ids = set.intersection(*sample_id_sets.values())\n",
    "print(\"Number of common Sample_IDs across all files:\", len(common_sample_ids))\n",
    "\n",
    "# Optionally, filter each file to retain only the common Sample_IDs\n",
    "for name, file_path in files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    before_count = df['Sample_ID'].nunique()\n",
    "    df_filtered = df[df['Sample_ID'].isin(common_sample_ids)].copy()\n",
    "    after_count = df_filtered['Sample_ID'].nunique()\n",
    "    print(f\"{name}: Before filtering = {before_count} unique Sample_IDs, After filtering = {after_count} unique Sample_IDs.\")\n",
    "    \n",
    "    # Save the filtered file (this will create new files with '_filtered' appended)\n",
    "    df_filtered.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59695936-4084-49cf-a650-3e3ccb9fc435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Data before cleaning: 0 missing values.\n",
      "Clinical Data after cleaning: 0 missing values.\n",
      "Clinical Data contains 199 unique Sample_IDs after cleaning.\n",
      "mRNA Data before cleaning: 0 missing values.\n",
      "mRNA Data after cleaning: 0 missing values.\n",
      "mRNA Data contains 199 unique Sample_IDs after cleaning.\n",
      "Methylation Data before cleaning: 0 missing values.\n",
      "Methylation Data after cleaning: 0 missing values.\n",
      "Methylation Data contains 199 unique Sample_IDs after cleaning.\n",
      "CNA Data before cleaning: 0 missing values.\n",
      "CNA Data after cleaning: 0 missing values.\n",
      "CNA Data contains 199 unique Sample_IDs after cleaning.\n",
      "All files have been cleaned to remove any missing values.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths for the filtered files (assuming these were saved previously)\n",
    "files = {\n",
    "    \"Clinical Data\": \"data_clinical_patient_preprocessed.csv\",\n",
    "    \"mRNA Data\": \"data_mrna_seq_v2_rsem.csv\",\n",
    "    \"Methylation Data\": \"data_methylation_hm27_hm450_merged.csv\",\n",
    "    \"CNA Data\": \"data_log2_cna.csv\"\n",
    "}\n",
    "\n",
    "for desc, file_path in files.items():\n",
    "    # Load the file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count total missing values in the file\n",
    "    total_missing_before = df.isnull().sum().sum()\n",
    "    print(f\"{desc} before cleaning: {total_missing_before} missing values.\")\n",
    "    \n",
    "    # Drop rows that contain any missing values\n",
    "    df_clean = df.dropna(how='any')\n",
    "    \n",
    "    # Count missing values after cleaning (should be zero)\n",
    "    total_missing_after = df_clean.isnull().sum().sum()\n",
    "    print(f\"{desc} after cleaning: {total_missing_after} missing values.\")\n",
    "    \n",
    "    # Optionally, print the number of unique Sample_IDs\n",
    "    if 'Sample_ID' in df_clean.columns:\n",
    "        unique_samples = df_clean['Sample_ID'].nunique()\n",
    "        print(f\"{desc} contains {unique_samples} unique Sample_IDs after cleaning.\")\n",
    "    \n",
    "    # Save the cleaned file (overwrite the original filtered file)\n",
    "    df_clean.to_csv(file_path, index=False)\n",
    "    \n",
    "print(\"All files have been cleaned to remove any missing values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c00fdae4-ed8e-412a-bbcf-c2ef493daf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival labels saved to 'survival_labels.csv'. Verify the file to ensure correctness.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to clinical data\n",
    "clinical_file = 'data_clinical_patient_preprocessed.csv'\n",
    "\n",
    "# Load clinical data\n",
    "clinical_data = pd.read_csv(clinical_file)\n",
    "\n",
    "# Extract only Sample_ID and Survival_Label\n",
    "survival_data = clinical_data[['Sample_ID', 'Survival_Label']]\n",
    "\n",
    "# Save it as an Excel file for verification\n",
    "survival_data.to_csv('survival_labels.csv', index=False)\n",
    "\n",
    "print(\"Survival labels saved to 'survival_labels.csv'. Verify the file to ensure correctness.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb295349-0e3d-4f83-9ab1-137ad590f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data_mrna_seq_v2_rsem.csv...\n",
      "Performing feature selection on data_mrna_seq_v2_rsem.csv...\n",
      "Selected 36 features out of 17507.\n",
      "Reduced dataset saved to mrna_selected.csv.\n",
      "\n",
      "Loading data_methylation_hm27_hm450_merged.csv...\n",
      "Performing feature selection on data_methylation_hm27_hm450_merged.csv...\n",
      "Selected 29 features out of 21710.\n",
      "Reduced dataset saved to methylation_selected.csv.\n",
      "\n",
      "Loading data_log2_cna.csv...\n",
      "Performing feature selection on data_log2_cna.csv...\n",
      "Selected 3 features out of 25128.\n",
      "Reduced dataset saved to cna_selected.csv.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "\n",
    "def perform_feature_selection(data_path, sample_id_column, survival_file, output_path, n_estimators=1000):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Boruta with Survival_Label.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: Path to the omics dataset (CSV file).\n",
    "    - sample_id_column: The column containing Sample IDs.\n",
    "    - survival_file: Path to the survival labels CSV.\n",
    "    - output_path: Path to save the reduced dataset.\n",
    "    - n_estimators: Number of trees in the random forest.\n",
    "    \"\"\"\n",
    "    # Load omics data\n",
    "    print(f\"Loading {data_path}...\")\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Load survival labels\n",
    "    survival_data = pd.read_csv(survival_file)\n",
    "\n",
    "    # Merge with survival labels\n",
    "    merged_data = pd.merge(data, survival_data, on=sample_id_column, how='inner')\n",
    "\n",
    "    # Ensure Survival_Label exists\n",
    "    if 'Survival_Label' not in merged_data.columns:\n",
    "        raise ValueError(\"Survival_Label not found after merging!\")\n",
    "\n",
    "    # Separate features, target, and sample IDs\n",
    "    sample_ids = merged_data[sample_id_column]\n",
    "    target = merged_data['Survival_Label']\n",
    "    features = merged_data.drop(columns=[sample_id_column, 'Survival_Label'])\n",
    "\n",
    "    # Convert target to binary if necessary\n",
    "    if target.dtype == 'object':\n",
    "        target = np.where(target == \"High-Risk\", 1, 0)  # Convert categorical labels to 0/1 if needed\n",
    "\n",
    "    # Initialize Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1, random_state=42)\n",
    "\n",
    "    # Initialize Boruta\n",
    "    boruta = BorutaPy(rf, n_estimators='auto', random_state=42, max_iter=1000)\n",
    "\n",
    "    # Fit Boruta on data\n",
    "    print(f\"Performing feature selection on {data_path}...\")\n",
    "    boruta.fit(features.values, target)\n",
    "\n",
    "    # Get selected features\n",
    "    selected_features = features.columns[boruta.support_]\n",
    "    print(f\"Selected {len(selected_features)} features out of {features.shape[1]}.\")\n",
    "\n",
    "    # Reduce dataset to selected features \n",
    "    reduced_data = merged_data[selected_features].copy()\n",
    "\n",
    "    # Now safely assign 'Sample_ID' and 'Survival_Label'\n",
    "    reduced_data.insert(0, sample_id_column, sample_ids.values)  # Ensures Sample_ID is the first column\n",
    "    reduced_data[\"Survival_Label\"] = target.values  # Ensure labels are properly assigned\n",
    "\n",
    "    # Save reduced dataset\n",
    "    reduced_data.to_csv(output_path, index=False)\n",
    "    print(f\"Reduced dataset saved to {output_path}.\\n\")\n",
    "\n",
    "    return reduced_data\n",
    "\n",
    "# Paths to omics datasets and survival labels\n",
    "omics_files = {\n",
    "    'mRNA': 'data_mrna_seq_v2_rsem.csv',\n",
    "    'Methylation': 'data_methylation_hm27_hm450_merged.csv',\n",
    "    'CNA': 'data_log2_cna.csv'\n",
    "}\n",
    "survival_file = 'survival_labels.csv'\n",
    "\n",
    "# Perform feature selection for each dataset\n",
    "for omics_type, file_path in omics_files.items():\n",
    "    output_file = f\"{omics_type.lower()}_selected.csv\"\n",
    "    try:\n",
    "        perform_feature_selection(file_path, 'Sample_ID', survival_file, output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {omics_type}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d2f8b-9602-4864-8c78-87bce022b99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
